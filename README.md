# Blueprint AI / ML Work Simulation Exercise

In this exercise you’ll design and implement a lightweight evaluation system for outputs generated by a large language model in a clinical context. The goal is to evaluate your abilities to work with AI technologies.

## Background

Our product uses LLMs to automate therapist workflows — like writing session summaries or clinical documentation. Outputs need to meet a higher standard of **clarity**, **clinical alignment**, and **tone**. In this exercise, you'll create a small evaluation harness that can help us measure these subjective quality dimensions.

## The Challenge

### 1. Define Evaluation Criteria
Pick **at least 3 quality dimensions** you believe are important for clinical outputs. Examples might include:
- Clarity & structure
- Appropriateness of tone
- Clinical accuracy or hallucination detection
- Completeness vs. conciseness

Explain your criteria in the README or code comments.

### 2. Implement the Harness

Using the outputs in `data/`, build a tool (a Python script or notebook is fine) that:
- Calculates a score or label for each sample along your dimensions
- Uses **at least one LLM-as-a-judge technique** (e.g. GPT-4 scoring with a system prompt)
- Outputs a summary table of per-sample and aggregate scores

### 3. Document Your Thinking

Include write-up (in code comments or the README) covering:
- Why you chose your dimensions
- Tradeoffs or known weaknesses in your approach
- How you might integrate this into a CI or model iteration loop

### Evaluation Criteria

Your solution will be evaluated on:
- Quality of evaluation design
- Clarity in reasoning 
- Effective communication of the solution and tradeoffs
- Implementation of the evaluation harness

### Time Expectations

Plan to spend about 4 hours to complete this exercise. Focus on designing your approach and clearly documenting your solution. 

## Project Structure

- `/data/intake_session_transcript.txt` - Sample intake session transcript
- `/data/intake_note.txt` - Intake session note
- `/data/progress_session_transcript.txt` - Sample session transcript
- `/data/progress_note.txt` - Sample progress note

## Submission

When you're done:
1. Push your changes to your fork of this repository and make it public
2. Send us the link to your repository
3. Include any additional notes or documentation about your implementation

Good luck!
